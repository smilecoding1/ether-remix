# Import required libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

# Load IMDB dataset
vocab_size = 10000  # Only consider the top 10,000 words
maxlen = 200        # Limit each review to 200 words
(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)

# Pad sequences to ensure uniform length
x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)

# Build the DNN model
model = keras.Sequential([
    layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)

# Evaluate on test data
loss, accuracy = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {accuracy:.2f}")

# Plot accuracy and loss
plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Validation Accuracy")
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Validation Loss")
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

# Word index decoding utility
word_index = keras.datasets.imdb.get_word_index()
index_word = {index + 3: word for word, index in word_index.items()}
index_word[0] = "<PAD>"
index_word[1] = "<START>"
index_word[2] = "<UNK>"
index_word[3] = "<UNUSED>"

def decode_review(sequence):
    return ' '.join([index_word.get(i, '?') for i in sequence])

# Clean decoding function (removes <PAD>, <START>)
def decode_clean_review(sequence):
    cleaned_tokens = [token for token in sequence if token > 3]
    return decode_review(cleaned_tokens)

# Sample predictions on test data
print("\n\nSample Predictions on Test Data:\n")
num_samples_to_show = 5
predictions = model.predict(x_test[:num_samples_to_show])

for i in range(num_samples_to_show):
    decoded = decode_clean_review(x_test[i])
    actual_label = "Positive" if y_test[i] == 1 else "Negative"
    predicted_label = "Positive" if predictions[i] >= 0.5 else "Negative"
    print(f"Review {i+1}:")
    print(decoded[:500] + "...\n")
    print(f"Actual Label: {actual_label}")
    print(f"Predicted Label: {predicted_label}")
    print("-" * 80)	
